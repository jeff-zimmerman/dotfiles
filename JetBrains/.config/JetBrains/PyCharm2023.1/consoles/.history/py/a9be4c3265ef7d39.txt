measure("apply", lambda: df_basketball['player_names_clean'].apply(lambda x: re.findall(r'\w+', x.lower())))
-. . -..- - / . -. - .-. -.--
df_basketball_ent[['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']] = \
    df_basketball[['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']]
-. . -..- - / . -. - .-. -.--
df_basketball_ent[['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']]
-. . -..- - / . -. - .-. -.--
df_basketball[['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']]
-. . -..- - / . -. - .-. -.--
df_basketball_ent.shape
-. . -..- - / . -. - .-. -.--
x = [1,2,3,4]
-. . -..- - / . -. - .-. -.--
list(x)
-. . -..- - / . -. - .-. -.--
x = ['asdf' 'asdf']
-. . -..- - / . -. - .-. -.--
y = ['sdfsdgf']
-. . -..- - / . -. - .-. -.--
sum(x, y)
-. . -..- - / . -. - .-. -.--
x = ['asdf', 'asdf']
-. . -..- - / . -. - .-. -.--
x+y
-. . -..- - / . -. - .-. -.--
df_basketball_ent['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']= \
    df_basketball[['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']]
-. . -..- - / . -. - .-. -.--
df_basketball_ent['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']= \
    df_basketball['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']
-. . -..- - / . -. - .-. -.--
df_basketball_ent[['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']]= \
    df_basketball['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']
-. . -..- - / . -. - .-. -.--
df_basketball_ent[['ROOKIE', 'AUTOGRAPH', 'CARD_NUM', 'MEMORABILIA', 'PRINT_RUN', 'ROOKIE']]= \
    df_basketball[['rookie_card', 'autographed', 'card_num', 'memorabilia', 'print_run', 'rookie_card']]
-. . -..- - / . -. - .-. -.--
for col in df_basketball.columns:
    print(df_basketball[col].shape)
    
-. . -..- - / . -. - .-. -.--
for col in df_basketball.columns:
    print(col, 
          df_basketball[col].shape)
    
-. . -..- - / . -. - .-. -.--
for col in df_basketball.columns:
    print(col, 
          df_basketball[col].dtype)
    
-. . -..- - / . -. - .-. -.--
for col in df_basketball_ent_ex.columns:
    print(col,df_basketball_ent_ex[col].dtype)
    
-. . -..- - / . -. - .-. -.--
for col in df_basketball_ent_ex.columns:
    print(col,df_basketball_ent_ex[col].dtypes)
    
-. . -..- - / . -. - .-. -.--
for col in df_basketball_ent_ex.columns:
    print(col,df_basketball_ent_ex[col].dtypes=='object')
    
-. . -..- - / . -. - .-. -.--
[df_eval_ent[col].dtype for col in df_eval_ent.columns]
-. . -..- - / . -. - .-. -.--
[df_eval_ent[col].dtype == 'O' for col in df_eval_ent.columns]
-. . -..- - / . -. - .-. -.--
df_cand_match.head()
-. . -..- - / . -. - .-. -.--
y
-. . -..- - / . -. - .-. -.--
u
-. . -..- - / . -. - .-. -.--
n
-. . -..- - / . -. - .-. -.--
f
-. . -..- - / . -. - .-. -.--
runfile('/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing/iterative_prototype.py', wdir='/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing')
-. . -..- - / . -. - .-. -.--
import sklearn
-. . -..- - / . -. - .-. -.--
sklearn.metrics.jaccard_score('asdf', 'asdfg')
-. . -..- - / . -. - .-. -.--
sklearn.metrics.jaccard_score('asdf', 'asdf')
-. . -..- - / . -. - .-. -.--
x = ['a', 'b']
-. . -..- - / . -. - .-. -.--
x.remove('a')
-. . -..- - / . -. - .-. -.--
df = pd.DataFrame{'A' : [1, 2], 'B':[3,4]}
-. . -..- - / . -. - .-. -.--
df = pd.DataFrame({'A' : [1, 2], 'B':[3,4]})
-. . -..- - / . -. - .-. -.--
df
-. . -..- - / . -. - .-. -.--
df.columns
-. . -..- - / . -. - .-. -.--
df.columns.remove('A')
-. . -..- - / . -. - .-. -.--
df.loc[0]
-. . -..- - / . -. - .-. -.--
df.loc[~0]
-. . -..- - / . -. - .-. -.--
x = df.loc[0]
-. . -..- - / . -. - .-. -.--
df.loc[~x]
-. . -..- - / . -. - .-. -.--
df.loc[x]
-. . -..- - / . -. - .-. -.--
x.drop('A')
-. . -..- - / . -. - .-. -.--
import pandas as pd
-. . -..- - / . -. - .-. -.--
b, pl, sets, eval = load_sets()
-. . -..- - / . -. - .-. -.--
pl['Aka'].to_list()
-. . -..- - / . -. - .-. -.--
pd.concat(pl['Aka'], pl['Player'])
-. . -..- - / . -. - .-. -.--
pd.concat(pl[['Aka']], pl[['Player']])
-. . -..- - / . -. - .-. -.--
pd.concat((pl['Aka'], pl['Player']))
-. . -..- - / . -. - .-. -.--
pd.concat((pl['Aka'], pl['Player'])).reset_index()
-. . -..- - / . -. - .-. -.--
pd.concat((pl['Aka'], pl['Player'])).to_list()
-. . -..- - / . -. - .-. -.--
import inspect
-. . -..- - / . -. - .-. -.--
inspect.currentframe()
-. . -..- - / . -. - .-. -.--
extracted_entities.rename({'PERSON':'POOP'})
-. . -..- - / . -. - .-. -.--
extracted_entities.rename({'TEAM':'POOP'})
-. . -..- - / . -. - .-. -.--
extracted_entities.rename(mapper={'TEAM':'POOP'})
-. . -..- - / . -. - .-. -.--
extracted_entities.rename(columns={'TEAM':'POOP'})
-. . -..- - / . -. - .-. -.--
df_basketball[['year_start', 'year_end']].astype(str)
-. . -..- - / . -. - .-. -.--
df_basketball[['year_start', 'year_end']].map(lambda x: f'{x:0.0f}')
-. . -..- - / . -. - .-. -.--
df_basketball[['year_start', 'year_end']].applymap(lambda x: f'{x:0.0f}')
-. . -..- - / . -. - .-. -.--
df_basketball_ent_ex['PLAYER'].dtypes
-. . -..- - / . -. - .-. -.--
df_eval_ent_ex['PLAYER'].dtypes
-. . -..- - / . -. - .-. -.--
str(None)
-. . -..- - / . -. - .-. -.--
pd.isna(None)
-. . -..- - / . -. - .-. -.--
x = pd.Series({'A':1, 'B': None})
-. . -..- - / . -. - .-. -.--
x
-. . -..- - / . -. - .-. -.--
pd.isna(x)
-. . -..- - / . -. - .-. -.--
pd.isna(x, how='any')
-. . -..- - / . -. - .-. -.--
pd.isna(x).any(axis=1)
-. . -..- - / . -. - .-. -.--
pd.isna(x).any()
-. . -..- - / . -. - .-. -.--
def title_jaccard_sim(row):
    title = row.loc['Auctiontitle']
    s = row.drop('Auctiontitle').iloc[0]
    if pd.isna(title) or pd.isna(s):
        return 0
    title = set(str(title).lower())
    s = set(str(s).lower())
    intersection = set_title & set_s
    union = set_title | set_s
    return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
df_cand_match[['Auctiontitle', col]].head(10).progress_apply(title_jaccard_sim, axis=1)
-. . -..- - / . -. - .-. -.--
df_cand_match[['Auctiontitle', 'title']].head(10)
-. . -..- - / . -. - .-. -.--
type(df_cand_match[['Auctiontitle', 'title']].head(10))
-. . -..- - / . -. - .-. -.--
df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim, axis=1)
-. . -..- - / . -. - .-. -.--
set('morning')
-. . -..- - / . -. - .-. -.--
def title_jaccard_sim(row):
    title = row.loc['Auctiontitle']
    s = row.drop('Auctiontitle').iloc[0]
    if pd.isna(title) or pd.isna(s):
        return 0
    title = str(title).lower()
    s = str(s).lower()
    set_title = set(title.replace(',', ' ').split())
    set_s = set(s.replace(',', ' ').split())
    intersection = set_title & set_s
    union = set_title | set_s
    return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
def time(f):
    t_start = time.now()
    f()
    return time.now() - t_start

-. . -..- - / . -. - .-. -.--
time.now()
-. . -..- - / . -. - .-. -.--
time.now
-. . -..- - / . -. - .-. -.--
time
-. . -..- - / . -. - .-. -.--
time()
-. . -..- - / . -. - .-. -.--
def time(f):
    t_start = time()
    f()
    return time() - t_start

-. . -..- - / . -. - .-. -.--
def time_func(f):
    t_start = time()
    f()
    return time() - t_start

-. . -..- - / . -. - .-. -.--
time_func((lambda: df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim, axis=1)))
-. . -..- - / . -. - .-. -.--
from time import time
-. . -..- - / . -. - .-. -.--
locals()
-. . -..- - / . -. - .-. -.--
locals.get('time')
-. . -..- - / . -. - .-. -.--
del time
-. . -..- - / . -. - .-. -.--
del locals().get('time')
-. . -..- - / . -. - .-. -.--
locals().get('time')
-. . -..- - / . -. - .-. -.--
time_func(f=lambda: df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim, axis=1))
-. . -..- - / . -. - .-. -.--
def time_func(f):
    t_start = time.time()
    f()
    return time.time - t_start

-. . -..- - / . -. - .-. -.--
import time
-. . -..- - / . -. - .-. -.--
def time_func(f):
    t_start = time.time()
    f()
    return time.time() - t_start

-. . -..- - / . -. - .-. -.--
def title_jaccard_sim2(row):
        title = row.loc['Auctiontitle']
        s = row.drop('Auctiontitle').iloc[0]
        if pd.isna(title) or pd.isna(s):
            return 0
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
def title_jaccard_sim3(row):
        title = row[0]
        s = row[1]
        if pd.isna(title) or pd.isna(s):
            return 0
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
def title_jaccard_sim3(row):
        title = row.iloc[0]
        s = row.iloc[1]
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
time_func(lambda: df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim3, axis=1))
-. . -..- - / . -. - .-. -.--
time_func(lambda: df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim2, axis=1))
-. . -..- - / . -. - .-. -.--
time_func(lambda: df_cand_match[['Auctiontitle', 'title']].head(10).progress_apply(title_jaccard_sim, axis=1))
-. . -..- - / . -. - .-. -.--
time_func(lambda: [title_jaccard_sim3(s1, s2) for s1, s2 in zip(df_cand_match['Auctiontitle'], df_cand_match['title'].head(10)]))
-. . -..- - / . -. - .-. -.--
def title_jaccard_sim4(title, s):
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

-. . -..- - / . -. - .-. -.--
time_func(lambda: [title_jaccard_sim4(s1, s2)  if (pd.notna(s1) and pd.notna(s2)) else 0 for s1, s2 in zip(df_cand_match['Auctiontitle'], df_cand_match['title'].head(10))])
-. . -..- - / . -. - .-. -.--
[title_jaccard_sim4(s1, s2)  if (pd.notna(s1) and pd.notna(s2)) else 0 for s1, s2 in zip(df_cand_match['Auctiontitle'], df_cand_match['title'].head(10))]
-. . -..- - / . -. - .-. -.--
'asdf'.contains('asdf')
-. . -..- - / . -. - .-. -.--
str.__contains__('asdf')
-. . -..- - / . -. - .-. -.--
str.__contains__('asdf', as)
-. . -..- - / . -. - .-. -.--
str.__contains__('asdf', 'as')
-. . -..- - / . -. - .-. -.--
str.__contains__('asdf', 'ab')
-. . -..- - / . -. - .-. -.--
x = ['a', 'b', 'c']
-. . -..- - / . -. - .-. -.--
[i for i in x if 'a' in i]
-. . -..- - / . -. - .-. -.--
import os
import re
from collections import defaultdict

### external libraries
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm
# from sklearn.metrics import precision_score, recall_score, f1_score

### NLP extermnal libraries
import spacy
from nltk.tokenize import wordpunct_tokenize
from pandas_dedupe import link_dataframes

from sklearn.ensemble import RandomForestClassifier

# import BaseClassifier from recordlinkage.base
from recordlinkage.base import BaseClassifier
from recordlinkage.adapters import SKLearnClassifier
from recordlinkage.datasets import binary_vectors

-. . -..- - / . -. - .-. -.--
from recordlinkage.datasets import binary_vectors
-. . -..- - / . -. - .-. -.--
binary_vectors(10000, 2000, return_links=True)
-. . -..- - / . -. - .-. -.--
features, links = binary_vectors(10000, 2000, return_links=True)
-. . -..- - / . -. - .-. -.--
features
-. . -..- - / . -. - .-. -.--
def rf_join(df_cand_match, save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    """
    # Balance matched and unmatched
    cand_groups = df_cand_match['index'].unique()
    contains_match = 0
    df_list = []
    print('Generating balanced dataset')
    for group_idx in cand_groups:
        group = df_cand_match.loc[df_cand_match['index'] == group_idx]
        true_dict = group.loc[group['MatchTitle'] == group['AuctionTitle']].to_dict()
        false_dict = group.loc[group['MatchTitle'] != group['AuctionTitle']].sample(n=1).to_dict()
        df_list.append(true_dict)
        df_list.append(false_dict)

    balanced_df = pd.DataFrame(df_list)

    train_set = balanced_df[[col for col in balanced_df.columns if 'sim' in col]].sample(frac=0.8, random_state=42)
    test_set = balanced_df[[col for col in balanced_df.columns if 'sim' in col]].drop(train_set.index)

    train_labels = train_set['AuctionTitle'] == train_set['MatchTitle']
    test_labels = test_set['AuctionTitle'] == test_set['MatchTitle']

    print('Training random forests')
    RF = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)
    RF.fit(train_set, train_labels)

    predictions = RF.predict(test_set)

    print('Validation Metrics')
    print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
    print(f'\tPrecision: {precision_score(test_labels, predictions)}')
    print(f'\tRecall: {recall_score(test_labels, predictions)}')
    print(f'\tF1 Score: {f1_score(test_labels, predictions)}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF

-. . -..- - / . -. - .-. -.--
any([df_cand_match['MatchTitle'] == df_cand_match['AuctionTitle']])
-. . -..- - / . -. - .-. -.--
[df_cand_match['MatchTitle'] == df_cand_match['AuctionTitle']]
-. . -..- - / . -. - .-. -.--
[df_cand_match['MatchTitle'] == df_cand_match['AuctionTitle']].any()
-. . -..- - / . -. - .-. -.--
(df_cand_match['MatchTitle'] == df_cand_match['AuctionTitle']).any()
-. . -..- - / . -. - .-. -.--
df_cand_match['index']
-. . -..- - / . -. - .-. -.--
df_cand_match['index'].unique()
-. . -..- - / . -. - .-. -.--
df_cand_match['index'].unique().size
-. . -..- - / . -. - .-. -.--
extracted_entities = extract_entities(nlp, df_basketball)
-. . -..- - / . -. - .-. -.--
df_cand_match.__name__
-. . -..- - / . -. - .-. -.--
df_cand_match.__ne__
-. . -..- - / . -. - .-. -.--
type(df_cand_match).__name__
-. . -..- - / . -. - .-. -.--
len(df_cand_match['AuctionTitle'].unique())
-. . -..- - / . -. - .-. -.--
df_cand_match['AuctionTitle'].unique()
-. . -..- - / . -. - .-. -.--
len(df_cand_match['AuctionTitle'])
-. . -..- - / . -. - .-. -.--
df_cand_match.groupby(by=['AuctionTitle', 'MatchTitle']).count()
-. . -..- - / . -. - .-. -.--
df_cand_match.groupby(by=['AuctionTitle', 'MatchTitle']).ngroups
-. . -..- - / . -. - .-. -.--
test_set.index
-. . -..- - / . -. - .-. -.--
df_test = df_cand_match.iloc[test_set.index]
-. . -..- - / . -. - .-. -.--
df_test
-. . -..- - / . -. - .-. -.--
df_test.groupby(by=['AuctionTitle', 'MatchTitle']).mean()
-. . -..- - / . -. - .-. -.--
df_cand_match[test_set.index]
-. . -..- - / . -. - .-. -.--
df_cand_match.iloc[test_set.index]
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self)
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self).__init__(*args, **kwargs)
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self).predict([0, 1, 0, 1, 0])
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self).fit([0, 1, 0, 1, 0] , [1])
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self).fit([[0, 1, 0, 1, 0]] , [1])
-. . -..- - / . -. - .-. -.--
super(RandomForestJoin, self).fit([[0, 1, 0, 1, 0]] , [[1]])
-. . -..- - / . -. - .-. -.--
debug_set['Predictions'] = debug_preds
-. . -..- - / . -. - .-. -.--
pred_mean.set_index('level_0')
-. . -..- - / . -. - .-. -.--
(0.5 >= 0.5).astype(int)
-. . -..- - / . -. - .-. -.--
pd.Series(0.5 >= 0.5).astype(int)
-. . -..- - / . -. - .-. -.--
test_labels.mean()
-. . -..- - / . -. - .-. -.--
extracted_entities[['AuctionTitle', 'EvalTitle']].apply(lambda x: len(x[0]) - len(x[1]))
-. . -..- - / . -. - .-. -.--
x = {'asd': 'asdf'}
-. . -..- - / . -. - .-. -.--
print(**x)
-. . -..- - / . -. - .-. -.--
**x
-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print(true_labels)
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print((true_labels == predictions).all())
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        predictions2 = model.predict_groups(df, thresh=thresh, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print((predictions2 == true_labels).all())
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        predictions2 = model.predict_groups(df, thresh=thresh, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print((predictions2 == true_labels))
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        predictions2 = model.predict_groups(df, thresh=thresh, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print((predictions2 == true_labels).astype(int).mean())
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        predictions2 = model.predict_groups(df, thresh=thresh, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print(predictions.index)
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.loc[predictions.index, 'labels']
        print((df.loc[predictions.index, 'EvalTitle'] == df.loc[predictions.index, 'MatchTitle']).all())
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        X = X.reset_index(drop=True)
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_mean = group.mean()[['Predictions']]

        if thresh:
            return (pred_mean['Predictions'] >= thresh).astype(int)
        else:
            return pred_mean['Predictions']

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        print(predictions.index)
        # true_labels = df.loc[predictions.index, 'labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        # print(predictions.index)
        true_labels = df.loc[predictions.index.to_list(), 'labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        print(predictions.index.to_list())
        true_labels = df.loc[predictions.index, 'labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        print(predictions.index)
        true_labels = df.loc[predictions.index, 'labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        print(predictions.index)
        true_labels = df.loc[pd.Series(predictions.index), 'labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        print(predictions.index)
        true_labels = df.iloc[predictions.index]['labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
thresh = 0.26
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.1
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.0
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.01
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.001
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.0001
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.00001
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.00001
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=None, group=False, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = 0.5, name: str = None):

    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.iloc[predictions.index]['labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        bin_predictions = predictions
        true_labels = df.loc[:, 'labels']
        thresh = None
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
    print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
    print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
    print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=None, group=False, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=None, group=False, n_estimators=1000, save_model=False)

-. . -..- - / . -. - .-. -.--
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)
-. . -..- - / . -. - .-. -.--
extracted_entities = extract_entities(nlp, df_basketball, n=1000, seed=0)
df_cand_match = join_by_distance(extracted_entities, df_basketball)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_mean = group.max()[['Predictions']].reset_index(drop=True)

        if thresh:
            return (pred_mean['Predictions'] >= thresh).astype(int)
        else:
            return pred_mean['Predictions']

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=None, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
df_cand_match = join_by_distance(extracted_entities, df_basketball)

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(df_cand_match, thresh=thresh, group=False, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_mean = group.mean()[['Predictions']].reset_index(drop=True)

        if thresh:
            return (pred_mean['Predictions'] >= thresh).astype(int)
        else:
            return pred_mean['Predictions']

-. . -..- - / . -. - .-. -.--
balanced_data = create_dataset(df_cand_match)
-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            group: bool = False,
            thresh: float = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping alporithm (mean score of all extracted entities)
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators, max_depth=2, random_state=0)
    if group:
        predict = RF.predict_groups
        kwargs = {'thresh': thresh, 'method': 'predict'}
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        kwargs = {}

    RF.fit(train_set, train_labels)
    predictions = predict(ts, **kwargs)

    print('Validation Metrics: test set')
    if thresh:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(balanced_data, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def join_by_distance(extracted_entities, df_basketball):
    """Attempt iterative join based on Em's naive approach"""

    df_basketball = df_basketball.iloc[extracted_entities.index]

    df_basketball_ent = df_basketball[['item_id', 'title', 'year_start', 'year_end', 'brand_name',
                                       'team_names_clean', 'player_names_clean']].rename(columns={'title': 'MatchTitle'})
    df_basketball_ent.loc[:, 'PLAYER'] = df_basketball['player_names_clean'].apply(lambda x: re.findall(r'\w+', x))

    # convert year to string
    df_basketball_ent.loc[:, ['year_start', 'year_end']] = df_basketball[['year_start', 'year_end']].applymap(
        lambda x: f'{x:0.0f}')

    join_fields = [
        # 'DATE',
        # 'BRAND_NAME',
        # 'manufacturer_name',
        'PLAYER',
        # 'TEAM',
        # 'AUTOGRAPH', # Not useful for matches, almost all null
        # 'MEMORABILIA', # same as above
        # 'ROOKIE' # Per Narendra: Everyone puts rookie in title
    ]

    # add relevant fields
    df_eval_ent = extracted_entities[['EvalTitle', 'AuctionTitle'] + join_fields].reset_index()

    # exploooooode
    df_basketball_ent_ex = df_basketball_ent.explode('PLAYER', ignore_index=True)
    df_eval_ent_ex = df_eval_ent.explode('PLAYER', ignore_index=True)
    df_eval_ent_ex = df_eval_ent_ex.drop(df_eval_ent_ex['PLAYER'].str.find(r'\d+'))

    # convert to lowercase strings from object type
    df_basketball_ent_ex['PLAYER'] = df_basketball_ent_ex['PLAYER'].str.lower()
    df_eval_ent_ex['PLAYER'] = df_eval_ent_ex['PLAYER'].str.lower()

    print('Merging dataframes')
    df_cand_match = pd.merge(df_basketball_ent_ex, df_eval_ent_ex, how='left', on='PLAYER') \
        .reset_index(drop=True).rename(columns={'index': 'unexp_index'})
    # .drop_duplicates(subset=['index', 'item_id']) \ ### COMES BEFORE reset_index WHEN UNCOMMENTED

    print(f'{len(df_cand_match)} matched entities!')

    def title_jaccard_sim(title, s):
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

    # find string similarity for all fields
    ignored_columns = ['item_id', 'player_names_clean', 'unexp_index', 'AuctionTitle', 'EvalTitle',  'MatchTitle']
    for col in df_cand_match.columns:
        if col in ignored_columns:
            continue
        # explode out remaining lists
        df_cand_match = df_cand_match.explode(col, ignore_index=True)
        # calculate similarity
        df_cand_match[f'{col}_sim'] = \
            [title_jaccard_sim(s1, s2) if (pd.notna(s1) and pd.notna(s2)) else 0
             for s1, s2 in
             tqdm(zip(df_cand_match['AuctionTitle'], df_cand_match[col]), desc=f'calculating similarity {col}',
                  total=len(df_cand_match[col]))]

    df_sim = df_cand_match[[col for col in df_cand_match.columns if 'sim' in col] + ['AuctionTitle', 'EvalTitle', 'MatchTitle','unexp_index']]

    return df_sim

-. . -..- - / . -. - .-. -.--
bool(-1)
-. . -..- - / . -. - .-. -.--
thresh = 0.1
model, (test_set, test_labels) = rf_join(balanced_data, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        return predictions
        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.max()[['Predictions']].reset_index(drop=True)

        if thresh:
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            group: bool = False,
            thresh: float = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping alporithm (mean score of all extracted entities)
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators, max_depth=2, random_state=0)
    if group:
        predict = RF.predict_groups
        kwargs = {'thresh': thresh, 'method': 'predict_proba'}
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        kwargs = {}

    RF.fit(train_set, train_labels)
    predictions = predict(ts, **kwargs)
    return None, (predictions, None)

    print('Validation Metrics: test set')
    if thresh:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            group: bool = False,
            thresh: float = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping alporithm (mean score of all extracted entities)
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators, max_depth=2, random_state=0)
    if group:
        predict = RF.predict_groups
        kwargs = {'thresh': thresh, 'method': 'predict_proba'}
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        kwargs = {}

    RF.fit(train_set, train_labels)
    predictions = predict(ts, **kwargs)

    print('Validation Metrics: test set')
    if thresh:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
thresh = 0.1
model, (test_set, test_labels) = rf_join(balanced_data, thresh=None, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
            print(predictions, thresh)
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.max()[['Predictions']].reset_index(drop=True)

        if thresh:
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.max()[['Predictions']].reset_index(drop=True)

        if thresh:
            print((pred_group['Predictions'] >= thresh).astype(int))
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            group: bool = False,
            thresh: float = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping alporithm (mean score of all extracted entities)
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators, max_depth=2, random_state=0)
    if group:
        predict = RF.predict_groups
        kwargs = {'thresh': thresh, 'method': 'predict_proba'}
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        kwargs = {}

    RF.fit(train_set, train_labels)
    predictions = predict(ts, **kwargs)
    print(predictions)

    print('Validation Metrics: test set')
    if thresh:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
model, (test_set, test_labels) = rf_join(balanced_data, thresh=thresh, group=True, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
import os
import re

### external libraries
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \
    roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay

### NLP extermnal libraries
import spacy
from nltk.tokenize import wordpunct_tokenize
from pandas_dedupe import link_dataframes

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

-. . -..- - / . -. - .-. -.--
def load_sets():
    ### Load spacy model

    ### Data sets to load
    df_basketball = pd.read_csv('data/pricing_items_basketball_cleaned.csv')
    df_aka_players = pd.read_excel('data/Set_Aka_and_Player_Aka.xlsx', sheet_name=0)
    df_aka_sets = pd.read_excel('data/Set_Aka_and_Player_Aka.xlsx', sheet_name=1)
    df_evaluation = pd.read_csv('data/Basketball_Sample_Data_Auction_Title_and_Matched_Title.csv')

    return df_basketball, df_aka_players, df_aka_sets, df_evaluation

-. . -..- - / . -. - .-. -.--
def preprocess_data(df_basketball, df_aka_players, df_evaluation):
    ### Load variables from data sets ###
    tqdm.pandas(desc='Loading data sets')
    df_evaluation['Auctiontitle'] = df_evaluation['Auctiontitle'].progress_apply(lambda x: x.strip())

    df_basketball['player_names_clean'].fillna('', inplace=True)
    df_basketball['team_names_clean'].fillna('', inplace=True)
    df_basketball['manufacturer_name'].fillna('', inplace=True)
    df_basketball['brand_name'].fillna('', inplace=True)
    df_aka_players['Player'].fillna('', inplace=True)
    df_aka_players['Aka'].fillna('', inplace=True)

    return df_evaluation

-. . -..- - / . -. - .-. -.--
def format_spacy(df_basketball: pd.DataFrame,
                 df_aka_sets: pd.DataFrame,
                 save_model: bool = False):
    ### Adding patterns to SpaCy model ###
    nlp = spacy.load("en_core_web_sm")

    # Add player pattern and make it case insensitive
    players = list(df_basketball['player_names_clean'].unique()) + \
              list(df_aka_players['Player'].unique()) + \
              list(df_aka_players['Aka'].unique()) + ['Lebron James']
    players = list(set([r for p in players for r in re.findall(r'\w+', p)]))
    ### Adding new patterns to SpaCy ###

    # Add player pattern and make it case insensitive
    player_patterns = [
        {'label': "PERSON", 'pattern': [
            {"LOWER": str.lower(p)}
        ]} for p in players
    ]

    # Add team pattern
    teams = list(df_basketball['team_names_clean'].unique())
    teams.remove('')
    team_patterns = [
        {'label': "TEAM", 'pattern': [[{"IS_ALPHA": True}, {"LOWER": str.lower(y)}] for y in teams if y]}
    ]

    # Add manufacturer pattern
    manufacturers = list(df_basketball['manufacturer_name'].unique())
    manufacturers.remove('')
    manufacturer_patterns = [
        {'label': "MANUFACTURER", 'pattern': [
            {"LOWER": str.lower(x)} for x in wordpunct_tokenize(y)
        ]} for y in manufacturers
    ]

    # Add brandname pattern
    brand_names = list(df_basketball['brand_name'].unique()) + \
                  list(df_aka_sets['brand'].unique())
    brand_names.remove('')
    brand_name_patterns = [
        {'label': "BRAND_NAME", 'pattern': [
            {"LOWER": str.lower(x)} for x in wordpunct_tokenize(y)
        ]} for y in brand_names
    ]

    # Add color pattern
    colors = [
        'Silver',
        'Blue',
        'Red',
        'Purple',
        'Disco',
        'White',
        'Gold',
        'Orange',
        'Pride'
        'Green',
        'Pink',
        'Black'
    ]
    color_patterns = [
        {'label': "COLOR", 'pattern': [
            {"LOWER": str.lower(x)} for x in wordpunct_tokenize(y)
        ]} for y in colors
    ]

    # Add rookie pattern
    rookie_pattern = [
        {'label': "ROOKIE", 'pattern': [{"LOWER": 'rookie'}]}
    ]

    # Add autograph pattern
    autograph_pattern = [
        {'label': "AUTOGRAPH", 'pattern': [{"LOWER": {"REGEX": 'autograph|auto|signatures'}}]}
    ]

    # Add memobrabilia pattern
    memorabilia_pattern = [
        {'label': "MEMORABILIA", 'pattern': [{"LOWER": {"REGEX": 'memorab[a-zA-Z]+'}}]}
    ]

    # Add card number pattern
    card_num_pattern = [
        {'label': "CARD_NUM", 'pattern': [{"TEXT": "#"}, {"LIKE_NUM": True}]}
    ]

    # Add print run pattern
    print_run_pattern = [
        {'label': "PRINT_RUN", 'pattern': [{"TEXT": {"REGEX": "/[0-9]+"}}]}
        # {'label': "PRINT_RUN", 'pattern': [{"TEXT": "/"}, {"LIKE_NUM": True}]}
    ]

    # Code spacy with patterns
    ruler = nlp.add_pipe("entity_ruler", before="ner")
    ruler.add_patterns(player_patterns
                       # + team_patterns + manufacturer_patterns + \
                       # brand_name_patterns + color_patterns + rookie_pattern + autograph_pattern + \
                       # memorabilia_pattern + print_run_pattern + card_num_pattern
                       )

    if save_model:
        output_dir = Path(__file__).parent / 'models/base_model'
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print("Saved model to", output_dir)

    return nlp

-. . -..- - / . -. - .-. -.--
def extract_entities(nlp,
                     df_basketball: pd.DataFrame,
                     n: int = 1000,
                     seed: int = 0):
    # Local lookups are much faster than global lookups
    from collections import defaultdict

    def preprocess(title):
        '''
        Same very minimal pre-processing to make sure spaCy patterns work
        '''
        # Swap slash in date with hypen so that print run and date don't get confused
        title = re.sub('([0-9]{4})/([0-9]{2})', r'\1-\2', title)
        title = title.lower()
        return title

    def extract(title):
        """Helper function to extract entities from a title. Applied to dataframe."""
        title = preprocess(title)
        sents = nlp(title)

        title_extracted_entities = defaultdict(list)
        for ee in sents.ents:
            # Add entities to dict of lists if not in there already
            title_extracted_entities[ee.label_].append(ee.text)
        title_all_entities = {lab: title_extracted_entities.get(lab) for lab in labels}

        return pd.Series(title_all_entities)

    # Generate evaluation set of 1000 titles from df_basketball (df_evaluation is too messy and missing info)
    df_eval = df_basketball[~df_basketball['ebay_title'].str.contains('WNBA').fillna(False)][
        ['ebay_title', 'title']].dropna(how='any')
    df_eval = df_eval.sample(n=n, frac=None if n else 1, random_state=seed)
    df_eval = df_eval.apply(lambda x: x.str.strip())

    # If adding entity_ruler, we need those labels. ML labels get added to ner
    labels = nlp.pipe_labels.get('ner')
    tqdm.pandas(desc="Extracting entities from auction titles.")
    extracted_entities = df_eval['ebay_title'].progress_apply(extract)
    # add ebay titles and ground truth matched title to dataframe
    extracted_entities.loc[:, 'AuctionTitle'] = df_eval['ebay_title']
    extracted_entities.loc[:, 'EvalTitle'] = df_eval['title']

    # Add fields to dataframe if not present in nlp pipeline
    extracted_entities = extracted_entities.rename(columns={'PERSON': 'PLAYER'})
    ### Uncomment if using more than PERSON field
    # all_fields = ['CARD_NUM', 'AUTOGRAPH', 'MEMORABILIA', 'ROOKIE', 'BRAND_NAME', 'PLAYER', 'TEAM', 'DATE']
    # for field in all_fields:
    #     if field not in extracted_entities.columns:
    #         extracted_entities[field] = None
    #
    # # Find team names. Small(ish) set of team names, so we can use regex
    # all_teams = df_basketball['team_names_clean'].replace('', np.nan).dropna().str.split(', ').explode().unique()
    # extracted_entities['TEAM'] = extracted_entities['Auctiontitle'] \
    #     .str.findall('|'.join(all_teams), flags=re.IGNORECASE) \
    #     .apply(lambda x: ', '.join(x)).replace('', None)

    return extracted_entities

-. . -..- - / . -. - .-. -.--
def do_join(extracted_entities, df_basketball):
    # required for huggingface tokenizers
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    # expand lists in dataframe to include all permutations of lists
    extracted_entities_ex = extracted_entities.copy()
    for col in extracted_entities_ex.columns:
        extracted_entities_ex = extracted_entities_ex.explode(col)

    # Formatting datasets for comparison
    # join year_start and year_end to make a single year column in df_basketball
    df_basketball['year'] = df_basketball['year_start'].astype('Int64').astype('str') \
                            + '-' \
                            + df_basketball['year_end'].astype('Int64').astype('str').str[-2:]

    # Change str/None to True/False in extracted_entities_ex columns
    extracted_entities_ex[['MEMORABILIA', 'AUTOGRAPH', 'ROOKIE']] = \
        extracted_entities_ex[['MEMORABILIA', 'AUTOGRAPH', 'ROOKIE']].applymap(pd.notna)

    # Renaming columns for join (original names come from df_evaluation)
    extracted_entities_ex_renamed = extracted_entities_ex.rename(columns={'AUTOGRAPH': 'autographed',
                                                                          'MEMORABILIA': 'memorabilia',
                                                                          'ROOKIE': 'rookie_card',
                                                                          'BRAND_NAME': 'manufacturer_name',
                                                                          'SET': 'brand_name',
                                                                          'PLAYER': 'player_names_clean',
                                                                          'TEAM': 'team_names_clean',
                                                                          'DATE': 'year'})
    # Do the join
    join_fields = [
        'year',
        # 'brand_name',
        # 'manufacturer_name',
        'player_names_clean',
        # 'team_names_clean',
        # 'autographed', # Not useful for matches, almost all null
        # 'memorabilia', # same as above
        # 'rookie_card'
    ]

    extracted_entities_ex_renamed = extracted_entities_ex_renamed[join_fields + ['MatchTitle', 'Auctiontitle']]
    df_basketball_join = df_basketball[join_fields + ['title', 'ebay_title']]

    # Drop rows with only date data for training
    if not os.path.exists('link_dataframes_learned_settings'):
        extracted_entities_ex_renamed = extracted_entities_ex_renamed \
            .dropna(subset=[
            # 'brand_name',
            'player_names_clean',
            # 'team_names_clean'
        ],
            how='all')
        # extracted_entities_ex_renamed = extracted_entities_ex_renamed \
        #                                     .loc[extracted_entities_ex_renamed[['rookie_card']].any(axis=1), :]
        extracted_entities_ex_renamed = extracted_entities_ex_renamed.replace('', None)

    # keep index of extracted_entities_ex_renamed for later use
    extracted_entities_ex_renamed = extracted_entities_ex_renamed.reset_index(drop=False)

    df_joined = link_dataframes(df_basketball_join, extracted_entities_ex_renamed, join_fields)

    return df_joined

-. . -..- - / . -. - .-. -.--
def join_by_distance(extracted_entities, df_basketball):
    """Attempt iterative join based on Em's naive approach"""

    df_basketball = df_basketball.iloc[extracted_entities.index]

    df_basketball_ent = df_basketball[['item_id', 'title', 'year_start', 'year_end', 'brand_name',
                                       'team_names_clean', 'player_names_clean']].rename(columns={'title': 'MatchTitle'})
    df_basketball_ent.loc[:, 'PLAYER'] = df_basketball['player_names_clean'].apply(lambda x: re.findall(r'\w+', x))

    # convert year to string
    df_basketball_ent.loc[:, ['year_start', 'year_end']] = df_basketball[['year_start', 'year_end']].applymap(
        lambda x: f'{x:0.0f}')

    join_fields = [
        # 'DATE',
        # 'BRAND_NAME',
        # 'manufacturer_name',
        'PLAYER',
        # 'TEAM',
        # 'AUTOGRAPH', # Not useful for matches, almost all null
        # 'MEMORABILIA', # same as above
        # 'ROOKIE' # Per Narendra: Everyone puts rookie in title
    ]

    # add relevant fields
    df_eval_ent = extracted_entities[['EvalTitle', 'AuctionTitle'] + join_fields].reset_index()

    # exploooooode
    df_basketball_ent_ex = df_basketball_ent.explode('PLAYER', ignore_index=True)
    df_eval_ent_ex = df_eval_ent.explode('PLAYER', ignore_index=True)

    # convert to lowercase strings from object type
    df_basketball_ent_ex['PLAYER'] = df_basketball_ent_ex['PLAYER'].str.lower()
    df_eval_ent_ex['PLAYER'] = df_eval_ent_ex['PLAYER'].str.lower()

    print('Merging dataframes')
    df_cand_match = pd.merge(df_basketball_ent_ex, df_eval_ent_ex, how='left', on='PLAYER') \
        .reset_index(drop=True).rename(columns={'index': 'unexp_index'})
    # .drop_duplicates(subset=['index', 'item_id']) \ ### COMES BEFORE reset_index WHEN UNCOMMENTED

    print(f'{len(df_cand_match)} matched entities!')

    def title_jaccard_sim(title, s):
        set_title = set(str(title).lower())
        set_s = set(str(s).lower())
        intersection = set_title & set_s
        union = set_title | set_s
        return len(intersection) / len(union)

    # find string similarity for all fields
    ignored_columns = ['item_id', 'player_names_clean', 'unexp_index', 'AuctionTitle', 'EvalTitle',  'MatchTitle']
    for col in df_cand_match.columns:
        if col in ignored_columns:
            continue
        # explode out remaining lists
        df_cand_match = df_cand_match.explode(col, ignore_index=True)
        # calculate similarity
        df_cand_match[f'{col}_sim'] = \
            [title_jaccard_sim(s1, s2) if (pd.notna(s1) and pd.notna(s2)) else 0
             for s1, s2 in
             tqdm(zip(df_cand_match['AuctionTitle'], df_cand_match[col]), desc=f'calculating similarity {col}',
                  total=len(df_cand_match[col]))]

    df_sim = df_cand_match[[col for col in df_cand_match.columns if 'sim' in col] + ['AuctionTitle', 'EvalTitle', 'MatchTitle','unexp_index']]

    return df_sim

-. . -..- - / . -. - .-. -.--
def create_dataset(df_cand_match):
    """helper function to make dataset"""
    cand_groups = df_cand_match['unexp_index'].unique()
    df_list = []
    for group_idx in tqdm(cand_groups, desc='Generating balanced dataset'):
        group = df_cand_match.loc[df_cand_match['unexp_index'] == group_idx]
        if (group['MatchTitle'] == group['EvalTitle']).any() and (group['MatchTitle'] != group['EvalTitle']).any():
            true_dict = group.loc[group['MatchTitle'] == group['EvalTitle']].to_dict(orient='records')[0]
            # false_dict = group.iloc[0].to_dict()
            false_dict = group.loc[group['MatchTitle'] != group['EvalTitle']].sample(n=1).to_dict(orient='records')[
                0]
            true_dict['labels'] = 1
            false_dict['labels'] = 0
            df_list.append(true_dict)
            df_list.append(false_dict)

    balanced_df = pd.DataFrame(df_list)

    train_set = balanced_df[[col for col in balanced_df.columns if 'sim' in col]+['labels']] \
        .sample(frac=0.8, random_state=42)
    test_set = balanced_df[[col for col in balanced_df.columns if 'sim' in col]+['AuctionTitle', 'EvalTitle', 'MatchTitle', 'labels']] \
        .drop(train_set.index)

    train_labels = train_set['labels']
    train_set = train_set.drop('labels', axis='columns')
    test_labels = test_set['labels']

    return (train_set, train_labels), (test_set, test_labels)


def log_join(df_cand_match, save_model=False):
    """
    Logistic regression Classifier for string distance matched dataframe
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = create_dataset(df_cand_match)

    print('Training logistic model')
    LR = LogisticRegression(random_state=42)
    LR.fit(train_set, train_labels)

    predictions = LR.predict(test_set)

    print(f'Validation Metrics: test_set')
    print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
    print(f'\tPrecision: {precision_score(test_labels, predictions)}')
    print(f'\tRecall: {recall_score(test_labels, predictions)}')
    print(f'\tF1 Score: {f1_score(test_labels, predictions)}')

    if save_model:
        joblib.dump(LR, 'models/scikit-learn/logistic.joblib')

    return LR, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        method = kwargs.pop('method', 'predict')
        thresh = kwargs.pop('thresh', 0.5)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if method == 'predict':
            predictions = self.predict(df_sim)
        elif method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.max()[['Predictions']].reset_index(drop=True)

        if thresh:
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            group: bool = False,
            thresh: float = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping alporithm (mean score of all extracted entities)
    """
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators, max_depth=2, random_state=0)
    if group:
        predict = RF.predict_groups
        kwargs = {'thresh': thresh, 'method': 'predict_proba'}
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        kwargs = {}

    RF.fit(train_set, train_labels)
    predictions = predict(ts, **kwargs)

    print('Validation Metrics: test set')
    if thresh or not group:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float|None = 0.5, name: str = None):

    print('Predicting...')
    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.iloc[predictions.index]['labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        bin_predictions = predictions
        true_labels = df.loc[:, 'labels']
        thresh = 1
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    if thresh:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
def assess_join(df_joined):
    """Function for metrics"""

    df_matched = df_joined[df_joined['cluster id'].notna()]
    n_matches = df_matched['cluster id'].drop_duplicates().size
    df_matched = df_matched.groupby('cluster id').agg(lambda x: set(x.dropna()))

    true_positive = pd.Series(df_matched.MatchTitle == df_matched.title)
    false_positive = pd.Series(df_matched.MatchTitle != df_matched.title)

    # They all have matches, so false negatives are difference between all and matched
    false_negative = len(df_joined) - len(df_matched)

    # Metrics
    accuracy = true_positive.sum() / len(df_joined)
    precision = true_positive.sum() / (true_positive.sum() + false_positive.sum())
    recall = true_positive.sum() / (true_positive.sum() + false_negative)
    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else None

    print(f"Total Matches: {n_matches}")
    print(f"Acc: {100 * accuracy}%")
    print(f"Precision: {100 * precision}%")
    print(f"Recall: {100 * recall}%")
    print(f"F1: {f1}")

    return df_matched, true_positive, false_positive


if __name__ == '__main__':
    #%%
    df_basketball, df_aka_players, df_aka_sets, df_evaluation = load_sets()
    df_evaluation = preprocess_data(df_basketball, df_aka_players, df_evaluation)
    nlp = format_spacy(df_basketball, df_aka_sets, save_model=False)
    # nlp = spacy.load('./models/spacy/trained_01/model-best')
    #%%
    extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
    df_cand_match = join_by_distance(extracted_entities, df_basketball)
    balanced_data = create_dataset(df_cand_match)
    #%%
    thresh = None
    model, (test_set, test_labels) = rf_join(balanced_data, group=False, thresh=thresh, n_estimators=1000, save_model=False)
    df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)
    #%%
    assess_model(model, df_cand_match, thresh=thresh, name='Full Candidate Matches')

-. . -..- - / . -. - .-. -.--
df_basketball, df_aka_players, df_aka_sets, df_evaluation = load_sets()
df_evaluation = preprocess_data(df_basketball, df_aka_players, df_evaluation)
nlp = format_spacy(df_basketball, df_aka_sets, save_model=False)
# nlp = spacy.load('./models/spacy/trained_01/model-best')

-. . -..- - / . -. - .-. -.--
x.pop('a')
-. . -..- - / . -. - .-. -.--
x.pop('s')
-. . -..- - / . -. - .-. -.--
x.pop('s', Non)
-. . -..- - / . -. - .-. -.--
x = {'s': 2}
-. . -..- - / . -. - .-. -.--
x.pop('s', None)
-. . -..- - / . -. - .-. -.--
x.pop('a', None)
-. . -..- - / . -. - .-. -.--
del x
-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float|None = 0.5, name: str = None):

    print('Predicting...')
    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.iloc[predictions.index]['labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        bin_predictions = predictions
        true_labels = df.loc[:, 'labels']
        thresh = 1
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    if thresh or model.group:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

        self.thresh = None
        self.method = 'predict'
        self.group = False

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        self.method = kwargs.pop('method', 'predict')
        self.thresh = kwargs.pop('thresh', None)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if self.method == 'predict':
            predictions = self.predict(df_sim)
        elif self.method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.mean()[['Predictions']].reset_index(drop=True)

        if self.thresh:
            self.group = True
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
thresh = None
model, (test_set, test_labels) = rf_join(balanced_data, group=False, thresh=thresh, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float|None = 0.5, name: str = None):

    print('Predicting...')
    if type(model).__name__ == 'RandomForestJoin':
        predictions = model.predict_groups(df, thresh=None, method='predict')
        bin_predictions = (predictions >= thresh).astype(int)
        true_labels = df.iloc[predictions.index]['labels']
    elif type(model).__name__ == 'RandomForestClassifier':
        predictions = model.predict(df[[col for col in df.columns if 'sim' in col]])
        bin_predictions = predictions
        true_labels = df.loc[:, 'labels']
        thresh = 1
    else:
        raise TypeError(f'Unknown model type: {type(model).__name__}')



    print(f'Validation Metrics {name}')
    if thresh or not model.group:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    roc_score = roc_auc_score(true_labels, predictions)
    print(f'\tROC-AUC Score: {roc_score}')

    fpr, tpr, _ = roc_curve(true_labels, predictions)
    prec, rec, _ = precision_recall_curve(true_labels, predictions)

    fig, (ax1, ax2) = plt.subplots(1, 2)
    pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
    pr_display.plot(ax=ax1)
    roc_display.plot(ax=ax2)
    ax1.set_title('Precision - Recall Curve')
    ax2.set_title('ROC Curve')
    fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
    plt.show()

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

        self.thresh = None
        self.method = 'predict'
        self.group = False

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        self.method = kwargs.pop('method', 'predict')
        self.thresh = kwargs.pop('thresh', None)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if self.method == 'predict':
            predictions = self.predict(df_sim)
        elif self.method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {self.method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.mean()[['Predictions']].reset_index(drop=True)

        if self.thresh:
            self.group = True
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
thresh = 0.5
model, (test_set, test_labels) = rf_join(balanced_data, group=True, thresh=thresh, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):
        super(RandomForestJoin, self).__init__(*args, **kwargs)

        self.thresh = None
        self.method = 'predict'
        self.group = False

    def predict_groups(self, X: pd.DataFrame, **kwargs):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        self.method = kwargs.pop('method', 'predict')
        self.thresh = kwargs.pop('thresh', None)
        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if self.method == 'predict':
            predictions = self.predict(df_sim)
        elif self.method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {self.method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])
        pred_group = group.max()[['Predictions']].reset_index(drop=True)

        if self.thresh:
            self.group = True
            return (pred_group['Predictions'] >= thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
thresh = 0.25
model, (test_set, test_labels) = rf_join(balanced_data, group=True, thresh=thresh, n_estimators=1000, save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, name: str = None):

    print('Predicting...')
    if model.group:
        predictions = model.predict_groups(df)
    else:
        predictions = model.predict(df)

    bin_predictions = (predictions >= thresh).astype(int)
    true_labels = df.iloc[predictions.index]['labels']


    print(f'Validation Metrics {name}')
    if model.thresh or not model.group:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    if not model.thresh:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='mean',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = None
method = 'predict'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='mean',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='mean',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
df_cand_match = join_by_distance(extracted_entities, df_basketball)
balanced_data = create_dataset(df_cand_match)

-. . -..- - / . -. - .-. -.--
thresh = None
method = 'predict_proba'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='mean',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
import os
import re

### external libraries
import time
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \
    roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay

### NLP extermnal libraries
import spacy
from nltk.tokenize import wordpunct_tokenize
from pandas_dedupe import link_dataframes

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, name: str = None):

    print('Predicting...')
    t_start = time.time()
    if model.group:
        predictions = model.predict_groups(df)
    else:
        predictions = model.predict(df)
    print(f'Prediction time: {time.time() - t_start}')

    bin_predictions = (predictions >= thresh).astype(int)
    true_labels = df.iloc[predictions.index]['labels']


    print(f'Validation Metrics {name}')
    if model.thresh or not model.group:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    if not model.thresh:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_join(df_joined):
    """Function for metrics"""

    df_matched = df_joined[df_joined['cluster id'].notna()]
    n_matches = df_matched['cluster id'].drop_duplicates().size
    df_matched = df_matched.groupby('cluster id').agg(lambda x: set(x.dropna()))

    true_positive = pd.Series(df_matched.MatchTitle == df_matched.title)
    false_positive = pd.Series(df_matched.MatchTitle != df_matched.title)

    # They all have matches, so false negatives are difference between all and matched
    false_negative = len(df_joined) - len(df_matched)

    # Metrics
    accuracy = true_positive.sum() / len(df_joined)
    precision = true_positive.sum() / (true_positive.sum() + false_positive.sum())
    recall = true_positive.sum() / (true_positive.sum() + false_negative)
    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else None

    print(f"Total Matches: {n_matches}")
    print(f"Acc: {100 * accuracy}%")
    print(f"Precision: {100 * precision}%")
    print(f"Recall: {100 * recall}%")
    print(f"F1: {f1}")

    return df_matched, true_positive, false_positive


if __name__ == '__main__':
    #%%
    df_basketball, df_aka_players, df_aka_sets, df_evaluation = load_sets()
    df_evaluation = preprocess_data(df_basketball, df_aka_players, df_evaluation)
    nlp = format_spacy(df_basketball, df_aka_sets, save_model=False)
    # nlp = spacy.load('./models/spacy/trained_01/model-best')
    #%%
    extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
    df_cand_match = join_by_distance(extracted_entities, df_basketball)
    balanced_data = create_dataset(df_cand_match)
    #%%
    thresh = None
    method = 'predict_proba'
    model, (test_set, test_labels) = rf_join(balanced_data,
                                             thresh=thresh,
                                             method=method,
                                             group_method='mean',
                                             n_estimators=1000,
                                             save_model=False)
    df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)
    #%%
    assess_model(model, df_cand_match, name='Full Candidate Matches')

-. . -..- - / . -. - .-. -.--
thresh = 0.3
method = 'predict_proba'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='max',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.3
method = 'predict'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='mean',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='max',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = None
method = 'predict_proba'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method='max',
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict'
group_method = 'mean'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
runfile('/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing/naive_ebay_record_linkage.py', wdir='/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing')
-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
group_method = 'mean'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):

        self.method = kwargs.pop('method', 'predict')
        self.thresh = kwargs.pop('thresh', None)
        self.group_method = kwargs.pop('group_method', None)
        self.isgrouped = bool(self.group_method) # used only as flag for metric functions

        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if self.method == 'predict':
            predictions = self.predict(df_sim)
        elif self.method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {self.method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])

        if self.group_method == 'mean':
            pred_group = group.mean()[['Predictions']].reset_index(drop=True)
        elif self.group_method == 'max':
            pred_group = group.max()[['Predictions']].reset_index(drop=True)
        else:
            raise ValueError(f'Group method must be of "mean" or "max". got {self.group_method}')

        if self.thresh:
            return (pred_group['Predictions'] >= self.thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
thresh = 0.3
method = 'predict_proba'
group_method = 'max'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)
    if RF.isgrouped:
        predict = RF.predict_groups
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]

    RF.fit(train_set, train_labels)
    predictions = predict(ts)
    return predictions, (None, None)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)
    if RF.isgrouped:
        predict = RF.predict_groups
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]

    RF.fit(train_set, train_labels)
    predictions = predict(ts)

    return predictions, (None, None)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
thresh = 0.
method = 'predict_proba'
group_method = 'max'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
group_method = 'max'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)
df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)
    if RF.isgrouped:
        predict = RF.predict_groups
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]

    RF.fit(train_set, train_labels)
    predictions = predict(ts)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, test_labels)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
group_method = 'max'
model, (test_set, test_labels) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()
    df_stripped = df[[col for col in df.columns if 'sim' in col]]
    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df_stripped)
    else:
        predictions = model.predict(df_stripped)
    print(f'Prediction time: {time.time() - t_start}')

    bin_predictions = (predictions >= model.thresh).astype(int)
    true_labels = df.iloc[predictions.index]['labels']


    print(f'Validation Metrics {name}')
    if model.thresh or not model.isgrouped:
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')

    if not model.thresh:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
assess_model(model, df_cand_match, name='Full Candidate Matches')
-. . -..- - / . -. - .-. -.--
predictions.dtypes
-. . -..- - / . -. - .-. -.--
class RandomForestJoin(RandomForestClassifier):
    """
    Subclass RandomForestClassifier to predict by ratio of matches in df.groupby(by=['AuctionTitle', 'MatchTitle']).
    This is because incorrect extracted entities will not be classified as matches and therefore skew accuracy.
    thresh (optional) returns binary classification
    """
    def __init__(self, *args, **kwargs):

        self.method = kwargs.pop('method', 'predict')
        self.thresh = kwargs.pop('thresh', None)
        self.group_method = kwargs.pop('group_method', None)
        self.isgrouped = bool(self.group_method) # used only as flag for metric functions

        super(RandomForestJoin, self).__init__(*args, **kwargs)

    def predict_groups(self, X: pd.DataFrame):
        """
        FULL DATAFRAME WITH AuctionTitle and MatchTitle
        """

        df_sim = X[[col for col in X.columns if 'sim' in col]]
        if self.method == 'predict':
            predictions = self.predict(df_sim)
        elif self.method == 'predict_proba':
            predictions = self.predict_proba(df_sim)[:, 1]
        else:
            raise ValueError(f'Method must be of "predict" or "predict_proba". got {self.method}')

        # return predictions
        X.loc[:, 'Predictions'] = predictions
        group = X[[X.columns[0]]+['EvalTitle', 'MatchTitle', 'Predictions']].groupby(by=['EvalTitle', 'MatchTitle'])

        if self.group_method == 'mean':
            pred_group = group.mean()[['Predictions']].reset_index(drop=True)
        elif self.group_method == 'max':
            pred_group = group.max()[['Predictions']].reset_index(drop=True)
        else:
            raise ValueError(f'Group method must be of "mean" or "max". got {self.group_method}')

        if self.thresh:
            print('in is self.thresh')
            return (pred_group['Predictions'] >= self.thresh).astype(int)
        else:
            return pred_group['Predictions']

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)
    if RF.isgrouped:
        print('isgrouped')
        predict = RF.predict_groups
        ts = test_set
    else:
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]

    RF.fit(train_set, train_labels)
    predictions = predict(ts)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)
    if RF.isgrouped:
        predict = RF.predict_groups
        ts = test_set
    else:
        print('notgrouped')
        predict = RF.predict
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]

    RF.fit(train_set, train_labels)
    predictions = predict(ts)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if group_method == 'predict_proba':
            predictions = RF.predict_proba(ts)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if thresh or not RF.isgrouped:
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()
    df_stripped = df[[col for col in df.columns if 'sim' in col]]
    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df_stripped)
    else:
        predictions = model.predict(df_stripped)
    print(f'Prediction time: {time.time() - t_start}')

    true_labels = df.iloc[predictions.index]['labels']

    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()


if __name__ == '__main__':
    #%%
    df_basketball, df_aka_players, df_aka_sets, df_evaluation = load_sets()
    df_evaluation = preprocess_data(df_basketball, df_aka_players, df_evaluation)
    nlp = format_spacy(df_basketball, df_aka_sets, save_model=False)
    # nlp = spacy.load('./models/spacy/trained_01/model-best')
    #%%
    extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
    df_cand_match = join_by_distance(extracted_entities, df_basketball)
    balanced_data = create_dataset(df_cand_match)
    #%%
    thresh = None
    method = 'predict_proba'
    group_method = None
    model, (test_set, predictions) = rf_join(balanced_data,
                                             thresh=thresh,
                                             method=method,
                                             group_method=group_method,
                                             n_estimators=1000,
                                             save_model=False)

    df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)
    #%%
    assess_model(model, test_set, thresh=thresh, name='Full Candidate Matches')

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(predictions)
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(predictions)
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        print(predictions)
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)[:, 1]
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        print(predictions)
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)[:, 1]
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
thresh = None
method = 'predict'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
bool(0.0)
-. . -..- - / . -. - .-. -.--
x.pop('a', 'b')
-. . -..- - / . -. - .-. -.--
try: x.pop('a')
except: print('b')

-. . -..- - / . -. - .-. -.--
x = {'a': None}
-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')

    true_labels = df.iloc[predictions.index]['labels']

    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()


if __name__ == '__main__':
    #%%
    df_basketball, df_aka_players, df_aka_sets, df_evaluation = load_sets()
    df_evaluation = preprocess_data(df_basketball, df_aka_players, df_evaluation)
    nlp = format_spacy(df_basketball, df_aka_sets, save_model=False)
    # nlp = spacy.load('./models/spacy/trained_01/model-best')
    #%%
    extracted_entities = extract_entities(nlp, df_basketball, n=10000, seed=0)
    df_cand_match = join_by_distance(extracted_entities, df_basketball)
    balanced_data = create_dataset(df_cand_match)
    #%%
    thresh = 0.5
    method = 'predict_proba'
    group_method = None
    model, (test_set, predictions) = rf_join(balanced_data,
                                             thresh=thresh,
                                             method=method,
                                             group_method=group_method,
                                             n_estimators=1000,
                                             save_model=False)

    df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)
    #%%
    assess_model(model, test_set, thresh=thresh, name='Full Candidate Matches')

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)[:, 1]
            if RF.thresh:
                predictions = (predictions >= RF.thresh).astype(int)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
def rf_join(balanced_data,
            n_estimators: int = 1000,
            thresh: float = None,
            method: str = 'predict',
            group_method: str = None,
            save_model=False):
    """
    Random Forest Classifier for string distance matched dataframe
    group: use grouping algorithm (mean score of all extracted entities)
    """
    # TODO: Fix flow of this, not great
    # Balance matched and unmatched
    (train_set, train_labels), (test_set, test_labels) = balanced_data

    print('Training random forests')
    RF = RandomForestJoin(n_estimators=n_estimators,
                          thresh=thresh,
                          method=method,
                          group_method=group_method,
                          max_depth=2, random_state=0)

    RF.fit(train_set, train_labels)

    # Print test set metrics
    if RF.isgrouped:
        ts = test_set
        predictions = RF.predict_groups(ts)
    else:
        ts = test_set[[col for col in test_set.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = RF.predict_proba(ts)[:, 1]
            if RF.thresh:
                print('asdf')
                predictions = (predictions >= RF.thresh).astype(int)
        else:
            predictions = RF.predict(ts)

    print('Validation Metrics: test set')
    if RF.thresh or RF.method == 'predict':
        print(f'\tAccuracy: {accuracy_score(test_labels, predictions)}')
        print(f'\tPrecision: {precision_score(test_labels, predictions)}')
        print(f'\tRecall: {recall_score(test_labels, predictions)}')
        print(f'\tF1 Score: {f1_score(test_labels, predictions)}')
    else:
        roc_score = roc_auc_score(test_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

    if save_model:
        joblib.dump(RF, 'models/scikit-learn/random_forest.joblib')

    return RF, (test_set, predictions)

-. . -..- - / . -. - .-. -.--
thresh = 0.5
method = 'predict_proba'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = pd.Series((predictions >= model.thresh).astype(int))
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')

    true_labels = df.iloc[predictions.index]['labels']

    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        true_labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        true_labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(true_labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(true_labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(true_labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(true_labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(true_labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, _ = roc_curve(true_labels, predictions)
        prec, rec, _ = precision_recall_curve(true_labels, predictions)

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, thresholds = roc_curve(labels, predictions)
        prec, rec, _ = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = thresholds[ix]
        print('Best Threshold=%f' % (best_thresh))

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
thresh = 0.569217
method = 'predict_proba'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.569217
method = 'predict_proba'
group_method = 'max'
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = 0.0
method = 'predict_proba'
group_method = 'max'
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'Youden\'s J Best Threshold=%f {best_thresh}')

        P = np.abs(prec - rec)
        ix = np.argmin(P)
        best_thresh = pr_thresholds[ix]
        print(f'Precision/Recall maximizing Threshold {best_thresh}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'Youden\'s J Best Threshold: {best_thresh}')
        print(f'\tTPR:{tpr[ix]}, FPR: {fpr[ix]} ')

        P = np.abs(prec - rec)
        ix = np.argmin(P)
        best_thresh = pr_thresholds[ix]
        print(f'Precision/Recall maximizing Threshold: {best_thresh}')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
thresh = 0.8788
method = 'predict_proba'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
thresh = None
method = 'predict_proba'
group_method = None
model, (test_set, predictions) = rf_join(balanced_data,
                                         thresh=thresh,
                                         method=method,
                                         group_method=group_method,
                                         n_estimators=1000,
                                         save_model=False)

df_cand_match.loc[:, 'labels'] = (df_cand_match['EvalTitle'] == df_cand_match['MatchTitle']).astype(int)

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'Youden\'s J Best Threshold: {best_thresh}')
        print(f'\tTPR:{tpr[ix]}, FPR: {fpr[ix]} ')

        P = np.abs(prec + rec)
        ix = np.argmax(P)
        best_thresh = pr_thresholds[ix]
        print(f'Precision/Recall maximizing Threshold: {best_thresh}')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'Validation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'Youden\'s J Best Threshold: {best_thresh}')
        print(f'\tTPR:{tpr[ix]}, FPR: {fpr[ix]} ')

        # This can be changed to prec - rec for slightly different metrics
        P = np.abs(prec + rec)
        ix = np.argmax(P)
        best_thresh = pr_thresholds[ix]
        print(f'Precision + Recall maximizing Threshold: {best_thresh}')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        P = np.abs(prec - rec)
        ix = np.argmax(P)
        best_thresh = pr_thresholds[ix]
        print(f'Precision - Recall maximizing Threshold: {best_thresh}')
        print('\t --- (The point at which precision and recall are most equal) ---')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...', end='')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'\nValidation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'\nYouden\'s J Best Threshold: {best_thresh}')
        print(f'\tTPR:{tpr[ix]}, FPR: {fpr[ix]} ')

        # This can be changed to prec - rec for slightly different metrics
        P = np.abs(prec + rec)
        ix = np.argmax(P)
        best_thresh = pr_thresholds[ix]
        print(f'\nPrecision + Recall maximizing Threshold: {best_thresh}')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        P = np.abs(prec - rec)
        ix = np.argmin(P)
        best_thresh = pr_thresholds[ix]
        print(f'\nPrecision - Recall maximizing Threshold: {best_thresh}')
        print('\t --- (The point at which precision and recall are most equal) ---')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
assess_model(model, df_cand_match, thresh=thresh, name='Full Candidate Matches')
-. . -..- - / . -. - .-. -.--
def assess_model(model, df, thresh: float = None, name: str = None):

    print('Predicting...', end='')
    t_start = time.time()

    if thresh:
        model.thresh = thresh

    if model.isgrouped:
        predictions = model.predict_groups(df)
        labels = df.iloc[predictions.index]['labels']
    else:
        df_stripped = df[[col for col in df.columns if 'sim' in col]]
        labels = df['labels']
        if method == 'predict_proba':
            predictions = model.predict_proba(df_stripped)[:, 1]
            if model.thresh:
                predictions = (predictions >= model.thresh).astype(int)
        else:
            predictions = model.predict(df_stripped)

    print(f'Prediction time: {time.time() - t_start}')



    print(f'\nValidation Metrics {name}')
    if model.thresh or model.method == 'predict':
        bin_predictions = (predictions >= model.thresh).astype(int)
        print(f'\tAccuracy: {accuracy_score(labels, bin_predictions)}')
        print(f'\tPrecision: {precision_score(labels, bin_predictions)}')
        print(f'\tRecall: {recall_score(labels, bin_predictions)}')
        print(f'\tF1 Score: {f1_score(labels, bin_predictions)}')
    else:
        roc_score = roc_auc_score(labels, predictions)
        print(f'\tROC-AUC Score: {roc_score}')

        fpr, tpr, roc_thresholds = roc_curve(labels, predictions)
        prec, rec, pr_thresholds = precision_recall_curve(labels, predictions)
        print('len equal', len(roc_thresholds) == len(pr_thresholds))
        # get the best threshold
        J = tpr - fpr
        ix = np.argmax(J)
        best_thresh = roc_thresholds[ix]
        print(f'\nYouden\'s J Best Threshold: {best_thresh}')
        print(f'\tTPR:{tpr[ix]}, FPR: {fpr[ix]} ')

        P = np.abs(prec + rec)
        ix = np.argmax(P)
        best_thresh = pr_thresholds[ix]
        print(f'\nPrecision + Recall maximizing Threshold: {best_thresh}')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        P = np.abs(prec - rec)
        ix = np.argmin(P)
        best_thresh = pr_thresholds[ix]
        print(f'\nPrecision - Recall maximizing Threshold: {best_thresh}')
        print('\t --- (The point at which precision and recall are most equal) ---')
        print(f'\tPrecision: {prec[ix]}, Recall: {rec[ix]}')

        fig, (ax1, ax2) = plt.subplots(1, 2)
        pr_display = PrecisionRecallDisplay(precision=prec, recall=rec)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)
        pr_display.plot(ax=ax1)
        roc_display.plot(ax=ax2)
        ax1.set_title('Precision - Recall Curve')
        ax2.set_title('ROC Curve')
        fig.suptitle(f'n_trees: {len(model.estimators_)}, thresh: {thresh}')
        plt.show()

-. . -..- - / . -. - .-. -.--
assess_model(model, test_set, thresh=thresh, name='Full Candidate Matches')
-. . -..- - / . -. - .-. -.--
extracted_entities = extract_entities(nlp, df_basketball, n=100000, seed=0)
df_cand_match = join_by_distance(extracted_entities, df_basketball)
balanced_data = create_dataset(df_cand_match)

-. . -..- - / . -. - .-. -.--
runfile('/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing/rf_prototype.py', wdir='/home/jazimmerman/PycharmProjects/plainspoken/beckett/ebay_parsing/artemis-ebay-parsing')